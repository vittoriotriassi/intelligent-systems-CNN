\chapter{Conclusion}

Let us briefly recap what has been shown during this report. The main focus has been on \textit{Convolutional Neural Networks}. Several tecniques and different approaches have been proposed to improve performance of CNNs. We have firstly talked about what CNNs are and what their building blocks are. Then, we introduced some methods whose aim was to improve their accuracy on test sets. We used a few datasets such as MNIST \citep{MNIST}, CIFAR10 \citep{CIFAR10and100}, CIFAR100 \citep{CIFAR10and100}, STL10 \citep{STL10}, ImageNet \citep{ImageNet12}, LFW \citep{LFW}, Outex \citep{Outex} and SVHN \citep{SVHN}. They all were really important to compare different results achieved using different network architectures. Let us now dive into what we have seen while discussing about these papers.\\ \\
It seems to be clear that a CNN can be created by choosing a bunch of different parameters. That is why many researchers came up with several different architectures. Some of them have been more concentrated on how deep the neural network was supposed to be, going from a few layers to thousands of them. On the other hand, others have been more focused on how to ease the convolution operation. Also several activation functions were proposed. It is worth to mention the classic sigmoid which is almost every time replaced by the ReLU today (or also its slight variant Leaky ReLU). Some of the studies covered, showed that in order to extract more complex features, a few tricks can be used. We mention the data augmentation that basically allows the model to better abstract extracted features. For instance, some basic operation such as cropping, flipping etc. have been performed on almost all the datasets mentioned before. This allows the model to capture also different angles and lighting conditions of the image provided as input to the classificator which increases a lot the final accuracy. Moreover, some comments have been made on the output layer of a CNN and how to increase nonlinearity in order to better discriminate final classes. We have seen that, despite in CNNs we do not have all neurons connected with their former layer, in the last layer we need to have this configuration. Some other approaches proposed to have sparser networks by performing some regularization operations such as dropout or batch normalization. We also saw different pooling functions, in order to better downsample the input image while going deeper in the network. Finally, we discussed about a hardware accelerator that could help to obtain faster CNNs in order to deploy them also on mobile systems which can not substain the computational power usually required to run these models.\\ \\
We can conclude by saying that several were the proposed methods for CNNs, but in the end, when dealing with a problem (in this case it is image classification/object detection) we need to reach a compromise. Some of these papers have pointed out that it does not make totally sense to go deeper with more layers if this means that we are complicating the structure of the network. Because it would only be a race where the most powerful hardware achieves better results. Instead, it would make more sense to improve the local block of the whole architecture, as we have seen by comparing some of the previous papers.