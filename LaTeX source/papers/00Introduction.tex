\chapter{Introduction}

The aim of the following report is to debate about the technology of \textit{Convolutional Neural Networks} by collecting a number of papers published from 2015 until 2019 on most famous digital libraries. More specifically, ten papers will be presented and organized into chapters which contain different approaches to improve performances on the subject under examination. Each paper will be discussed and divided into two sections. The first one will deal with the proposed method, while the second one will cover experimental results and achievements. Finally, in the last chapter of our discussion, we will draw conclusions. Before going any further, an overview on CNNs should be provided.

\section{CNNs' Overview}

Convolutional Neural Networks are a type of Neural Networks that are widely used in computer vision. They have become very popular because they can extract several features from images and have a very good accuracy when it comes to deal with image classification and object recognition. Basically it is possible to extract complex features by learning them from low-level to high-level features. More specifically, we go from learning edges, dots and textures to shapes that have a semantic meaning.\\ \\
At  this  point,  we  might  ask  ourselves,  what  is  the  difference  between a  CNN  and a common Neural Network. First of all, a CNN assumes that its input is an image. Of course we might also provide something different than images, but generally, if we are interested in learning something that needs to take account of locality, there is a good chance that we will use a CNN. Otherwise it would be more useful to pick another type of network. A Convolutional Neural Network is made of something that we might define as building blocks. They all are layers but if we want to be more specific they are going to be \textit{Convolutional Layer, Pooling Layer} and \textit{Fully-Connected Layer}. The main difference between a CNN and an ordinary NN is that each neuron in a CNN is not connected to all the neurons in the previous layer. So, we do not have a fully-connected approach but shared weights that will allow our model to have much less parameters. The key element in CNNs is the filter, which sometimes is also called kernel or patch. It represents a small matrix that usually is of size $3 \times 3$ or $5 \times 5$. Its purpose is to perform the convolution operation, which is the core of this kind of network. This operation is a dot product between the filter and the input image and is performed in the convolutional layer. We slide this window from the left side of our image to the right side until we cover all the image bearing in mind what is the size of our original image for a possible padding. We end up with another matrix that represents the result of the convolution. In CNNs we generally want to learn many different filters and each of these is going to be more sensitive to a particular feature. It is important to point out that in CNNs we have also a third dimension that is the \textit{depth}. Thus, we have layers of size $width \times height \times depth$. Having said this, we come up with stacking these convolutional layers, and for a fixed number of these, we might decide whether or not stack a pooling layer, whose aim is to downsample the original image by controlling the overfitting and reducing the number of parameters. Even though it needs to be said that a lot of research support the idea that pooling is not that useful so in the end it is up to us. Finally we have the fully-connected layers that follow the same logic as an ordinary neural networks. That is, all the neurons are connected to the previous layer. A fully-connected layer is also called ``output layer'' and if we are dealing with a classification problem, it returns the class probabilities.

